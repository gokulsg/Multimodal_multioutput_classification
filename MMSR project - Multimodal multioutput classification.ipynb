{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Title</b>: MMSR project - Analysis of Memes\n",
    "\n",
    "<b>Author</b>: Gokul Srinivasagan\n",
    "\n",
    "<b>Date</b>: 12 September 2022\n",
    "\n",
    "<b>Version</b>: 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>text_ocr</th>\n",
       "      <th>text_corrected</th>\n",
       "      <th>humour</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>offensive</th>\n",
       "      <th>motivational</th>\n",
       "      <th>overall_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_1.jpg</td>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>general</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_2.jpeg</td>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>general</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_3.JPG</td>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_4.png</td>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_5.png</td>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>very_twisted</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>image_6988.jpg</td>\n",
       "      <td>Tuesday is Mardi Gras Wednesday is Valentine's...</td>\n",
       "      <td>Tuesday is Mardi Gras Wednesday is Valentine's...</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>image_6989.jpg</td>\n",
       "      <td>MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...</td>\n",
       "      <td>MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...</td>\n",
       "      <td>funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>image_6990.png</td>\n",
       "      <td>LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...</td>\n",
       "      <td>LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...</td>\n",
       "      <td>funny</td>\n",
       "      <td>general</td>\n",
       "      <td>slight</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>image_6991.jpg</td>\n",
       "      <td>When I VERY have time is a fantasy No one has ...</td>\n",
       "      <td>When I have time is a fantasy. no one has time...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>image_6992.jpg</td>\n",
       "      <td>The starting point for every good idea is \"Wha...</td>\n",
       "      <td>The starting point for every good idea is \"Wha...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6992 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_name                                           text_ocr  \\\n",
       "0        image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
       "1       image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n",
       "2        image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n",
       "3        image_4.png              10 Year Challenge - Sweet Dee Edition   \n",
       "4        image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n",
       "...              ...                                                ...   \n",
       "6987  image_6988.jpg  Tuesday is Mardi Gras Wednesday is Valentine's...   \n",
       "6988  image_6989.jpg  MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...   \n",
       "6989  image_6990.png  LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...   \n",
       "6990  image_6991.jpg  When I VERY have time is a fantasy No one has ...   \n",
       "6991  image_6992.jpg  The starting point for every good idea is \"Wha...   \n",
       "\n",
       "                                         text_corrected      humour  \\\n",
       "0     LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   hilarious   \n",
       "1     The best of #10 YearChallenge! Completed in le...   not_funny   \n",
       "2     Sam Thorne @Strippin ( Follow Follow Saw every...  very_funny   \n",
       "3                 10 Year Challenge - Sweet Dee Edition  very_funny   \n",
       "4     10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   hilarious   \n",
       "...                                                 ...         ...   \n",
       "6987  Tuesday is Mardi Gras Wednesday is Valentine's...  very_funny   \n",
       "6988  MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...       funny   \n",
       "6989  LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...       funny   \n",
       "6990  When I have time is a fantasy. no one has time...   not_funny   \n",
       "6991  The starting point for every good idea is \"Wha...   not_funny   \n",
       "\n",
       "              sarcasm       offensive      motivational overall_sentiment  \n",
       "0             general   not_offensive  not_motivational     very_positive  \n",
       "1             general   not_offensive      motivational     very_positive  \n",
       "2       not_sarcastic   not_offensive  not_motivational          positive  \n",
       "3     twisted_meaning  very_offensive      motivational          positive  \n",
       "4        very_twisted  very_offensive  not_motivational           neutral  \n",
       "...               ...             ...               ...               ...  \n",
       "6987  twisted_meaning  very_offensive      motivational           neutral  \n",
       "6988  twisted_meaning   not_offensive  not_motivational           neutral  \n",
       "6989          general          slight  not_motivational          positive  \n",
       "6990  twisted_meaning   not_offensive      motivational     very_positive  \n",
       "6991    not_sarcastic   not_offensive      motivational          positive  \n",
       "\n",
       "[6992 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the dataset #\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('memotion_dataset_7k/labels.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_corrected'].isnull().sum() ## 5 null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>text_ocr</th>\n",
       "      <th>text_corrected</th>\n",
       "      <th>humour</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>offensive</th>\n",
       "      <th>motivational</th>\n",
       "      <th>overall_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_1.jpg</td>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>general</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_2.jpeg</td>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>general</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_3.JPG</td>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_4.png</td>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_5.png</td>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>very_twisted</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>image_6988.jpg</td>\n",
       "      <td>Tuesday is Mardi Gras Wednesday is Valentine's...</td>\n",
       "      <td>Tuesday is Mardi Gras Wednesday is Valentine's...</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>image_6989.jpg</td>\n",
       "      <td>MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...</td>\n",
       "      <td>MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...</td>\n",
       "      <td>funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>image_6990.png</td>\n",
       "      <td>LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...</td>\n",
       "      <td>LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...</td>\n",
       "      <td>funny</td>\n",
       "      <td>general</td>\n",
       "      <td>slight</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>image_6991.jpg</td>\n",
       "      <td>When I VERY have time is a fantasy No one has ...</td>\n",
       "      <td>When I have time is a fantasy. no one has time...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>image_6992.jpg</td>\n",
       "      <td>The starting point for every good idea is \"Wha...</td>\n",
       "      <td>The starting point for every good idea is \"Wha...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6987 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_name                                           text_ocr  \\\n",
       "0        image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
       "1       image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n",
       "2        image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n",
       "3        image_4.png              10 Year Challenge - Sweet Dee Edition   \n",
       "4        image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n",
       "...              ...                                                ...   \n",
       "6987  image_6988.jpg  Tuesday is Mardi Gras Wednesday is Valentine's...   \n",
       "6988  image_6989.jpg  MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...   \n",
       "6989  image_6990.png  LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...   \n",
       "6990  image_6991.jpg  When I VERY have time is a fantasy No one has ...   \n",
       "6991  image_6992.jpg  The starting point for every good idea is \"Wha...   \n",
       "\n",
       "                                         text_corrected      humour  \\\n",
       "0     LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   hilarious   \n",
       "1     The best of #10 YearChallenge! Completed in le...   not_funny   \n",
       "2     Sam Thorne @Strippin ( Follow Follow Saw every...  very_funny   \n",
       "3                 10 Year Challenge - Sweet Dee Edition  very_funny   \n",
       "4     10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   hilarious   \n",
       "...                                                 ...         ...   \n",
       "6987  Tuesday is Mardi Gras Wednesday is Valentine's...  very_funny   \n",
       "6988  MUST WATCH MOVIES OF 2017 ITI Chennai memes MA...       funny   \n",
       "6989  LESS MORE TALKING PLANNING SODA JUNK FOOD COMP...       funny   \n",
       "6990  When I have time is a fantasy. no one has time...   not_funny   \n",
       "6991  The starting point for every good idea is \"Wha...   not_funny   \n",
       "\n",
       "              sarcasm       offensive      motivational overall_sentiment  \n",
       "0             general   not_offensive  not_motivational     very_positive  \n",
       "1             general   not_offensive      motivational     very_positive  \n",
       "2       not_sarcastic   not_offensive  not_motivational          positive  \n",
       "3     twisted_meaning  very_offensive      motivational          positive  \n",
       "4        very_twisted  very_offensive  not_motivational           neutral  \n",
       "...               ...             ...               ...               ...  \n",
       "6987  twisted_meaning  very_offensive      motivational           neutral  \n",
       "6988  twisted_meaning   not_offensive  not_motivational           neutral  \n",
       "6989          general          slight  not_motivational          positive  \n",
       "6990  twisted_meaning   not_offensive      motivational     very_positive  \n",
       "6991    not_sarcastic   not_offensive      motivational          positive  \n",
       "\n",
       "[6987 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Removing rows with Nan values ##\n",
    "\n",
    "df.dropna(subset=['text_corrected', 'humour', 'sarcasm', 'offensive', 'motivational', 'overall_sentiment'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_1567.jpg\n",
      "image_4924.jpg\n",
      "image_5119.png\n",
      "image_6357.jpg\n"
     ]
    }
   ],
   "source": [
    "## Checking if all the images can be readable ##\n",
    "\n",
    "import cv2\n",
    "\n",
    "image_names = df.image_name.values\n",
    "\n",
    "for img in image_names:\n",
    "    img_tensor = cv2.imread('./memotion_dataset_7k/images/'+img)\n",
    "    if img_tensor is not None:\n",
    "        #print(img_tensor.shape)\n",
    "        pass\n",
    "    else:\n",
    "        print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping all four memes which are not readable ##\n",
    "\n",
    "df = df[(df['image_name'] != 'image_1567.jpg') & (df['image_name'] !=  'image_4924.jpg') & (df['image_name'] != 'image_5119.png') & (df['image_name'] != 'image_6357.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6983, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "## Sanity check ##\n",
    "\n",
    "## Checking if all the images can be readable ##\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_names = df.image_name.values\n",
    "img_arr = []\n",
    "for img in image_names:\n",
    "    img_tensor = cv2.imread('./memotion_dataset_7k/images/'+img)\n",
    "    if img_tensor is not None:\n",
    "        img_tensor = cv2.resize(img_tensor,(224,224)) ## resizing to desired shape\n",
    "        img_arr.append(img_tensor)\n",
    "        pass\n",
    "    else:\n",
    "        print(img)\n",
    "        \n",
    "img_arr = np.array(img_arr)\n",
    "print(img_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing the cleaned dataset ##\n",
    "\n",
    "df.to_csv('./data/pre_processed_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total image in the dataset - 6983 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Spliting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with 20% values of original dataframe #\n",
    "val_test_df = df.sample(frac = 0.2)\n",
    " \n",
    "# Creating dataframe with rest of the 80% values #\n",
    "train_df = df.drop(val_test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a val and test df with 50% #\n",
    "\n",
    "val_df = val_test_df.sample(frac = 0.5)\n",
    "test_df = val_test_df.drop(val_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the files ##\n",
    "\n",
    "train_df.to_csv('./data/train.csv', index='False')\n",
    "val_df.to_csv('./data/val.csv', index='False')\n",
    "test_df.to_csv('./data/test.csv',index='False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading from the saved datasets ##\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "val_df = pd.read_csv('./data/val.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting data ##\n",
    "\n",
    "# Get the lists of sentences and their labels #\n",
    "\n",
    "sentences = train_df.text_corrected.values\n",
    "\n",
    "image_names = train_df.image_name.values\n",
    "\n",
    "label_humour = train_df.humour.values\n",
    "label_sarcasm = train_df.sarcasm.values\n",
    "label_offensive = train_df.offensive.values\n",
    "label_motivational = train_df.motivational.values\n",
    "label_overall_sentiment = train_df.overall_sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5586\n",
      "5586\n",
      "5586\n",
      "5586\n",
      "5586\n"
     ]
    }
   ],
   "source": [
    "# Converting the labels to integers #\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "label_humour_val = []\n",
    "\n",
    "for i in label_humour:\n",
    "    if i == 'not_funny':\n",
    "        label_humour_val.append(0)\n",
    "    elif i == 'funny':\n",
    "        label_humour_val.append(1)\n",
    "    elif i == 'very_funny':\n",
    "        label_humour_val.append(2)\n",
    "    elif i == 'hilarious':\n",
    "        label_humour_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_sarcasm_val = []\n",
    "\n",
    "for i in label_sarcasm:\n",
    "    if i == 'not_sarcastic':\n",
    "        label_sarcasm_val.append(0)\n",
    "    elif i == 'general':\n",
    "        label_sarcasm_val.append(1)\n",
    "    elif i == 'twisted_meaning':\n",
    "        label_sarcasm_val.append(2)\n",
    "    elif i == 'very_twisted':\n",
    "        label_sarcasm_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_offensive_val = []\n",
    "\n",
    "for i in label_offensive:\n",
    "    if i == 'not_offensive':\n",
    "        label_offensive_val.append(0)\n",
    "    elif i == 'slight':\n",
    "        label_offensive_val.append(1)\n",
    "    elif i == 'very_offensive':\n",
    "        label_offensive_val.append(2)\n",
    "    elif i == 'hateful_offensive':\n",
    "        label_offensive_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_motivational_val = []\n",
    "\n",
    "for i in label_motivational:\n",
    "    if i == 'not_motivational':\n",
    "        label_motivational_val.append(0)\n",
    "    elif i == 'motivational':\n",
    "        label_motivational_val.append(1)\n",
    "        \n",
    "\n",
    "label_overall_sentiment_val = []\n",
    "\n",
    "for i in label_overall_sentiment:\n",
    "    if i == 'very_negative':\n",
    "        label_overall_sentiment_val.append(0)\n",
    "    elif i == 'negative':\n",
    "        label_overall_sentiment_val.append(1)\n",
    "    elif i == 'neutral':\n",
    "        label_overall_sentiment_val.append(2)\n",
    "    elif i == 'positive':\n",
    "        label_overall_sentiment_val.append(3)\n",
    "    elif i == 'very_positive':\n",
    "        label_overall_sentiment_val.append(4)\n",
    "        \n",
    "        \n",
    "## Converting to numpy array ##\n",
    "label_humour_val = np.array(label_humour_val)\n",
    "label_sarcasm_val = np.array(label_sarcasm_val)\n",
    "label_offensive_val = np.array(label_offensive_val)\n",
    "label_motivational_val = np.array(label_motivational_val)\n",
    "label_overall_sentiment_val = np.array(label_overall_sentiment_val)\n",
    "\n",
    "## checking the dimensions -sanity check ##\n",
    "print(len(label_humour_val))\n",
    "print(len(label_sarcasm_val))\n",
    "print(len(label_offensive_val))\n",
    "print(len(label_motivational_val))\n",
    "print(len(label_overall_sentiment_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All the labels have same dimension ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='overall_sentiment'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['overall_sentiment']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='motivational'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['motivational']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='offensive'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['offensive']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='humour'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['humour']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sarcasm'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['sarcasm']).size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  261\n"
     ]
    }
   ],
   "source": [
    "## Checking the maximum length of the sentence ##\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "max_len = 0\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "for sent in sentences:\n",
    "    # Tokenize the text and add [CLS] and [SEP] tokens #\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length#\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing the image ##\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "preprocessed_image = []\n",
    "\n",
    "for img in image_names:\n",
    "    \n",
    "    img_tensor = cv2.imread('./memotion_dataset_7k/images/'+img)\n",
    "    img_tensor = cv2.resize(img_tensor,(224,224)) ## commonly used dimensison - 224\n",
    "    img_tensor = cv2.cvtColor(img_tensor, cv2.COLOR_BGR2RGB) ## format used by pytorch\n",
    "    # converting the type of pixel to float 32\n",
    "    img_tensor = img_tensor.astype('float32')\n",
    "    img_tensor = transform(img_tensor)\n",
    "    preprocessed_image.append(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,df): ## here df is not used ##\n",
    "\n",
    "        self.sentiment = [label for label in label_overall_sentiment_val] ## instead of having the lists here, the best option is use from dataframe like df['humour'] ##\n",
    "        self.humour = [label for label in label_humour_val]\n",
    "        self.sarcasm = [label for label in label_sarcasm_val]\n",
    "        self.offensive = [label for label in label_offensive_val]\n",
    "        self.motivational = [label for label in label_motivational_val]\n",
    "        \n",
    "        \n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 280, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in sentences] # Since maximum length is 261 # \n",
    "        self.image_name = [name for name in image_names]\n",
    "        self.images = [img for img in preprocessed_image]\n",
    "                                                                            \n",
    "    def classes(self):\n",
    "        return self.sentiment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.sentiment[idx]), np.array(self.humour[idx]), np.array(self.sarcasm[idx]), np.array(self.offensive[idx]), np.array(self.motivational[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of input texts\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def get_batch_images(self, idx):\n",
    "        # Fetch a batch of input images\n",
    "        return self.images[idx]\n",
    "    \n",
    "    def get_batch_image_names(self, idx):\n",
    "        # Fetch a batch of input images\n",
    "        return self.image_name[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_images = self.get_batch_images(idx)\n",
    "        batch_image_names = self.get_batch_image_names(idx)\n",
    "        \n",
    "        batch_y_sentiment, batch_y_humour, batch_y_sarcasm, batch_y_offensive, batch_y_motivational = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_images, batch_image_names, batch_y_sentiment, batch_y_humour, batch_y_sarcasm, batch_y_offensive, batch_y_motivational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5586 698 699\n"
     ]
    }
   ],
   "source": [
    "## Spliting the data into train, val and test ##\n",
    "\n",
    "# np.random.seed(112)\n",
    "# df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "# print(len(df_train),len(df_val), len(df_test)) ## Data split - train:val:test = 80:10:10 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5586"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train dataset and dataloader ##\n",
    "\n",
    "train = Dataset(train_df)\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True) #batch size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698\n",
      "698\n",
      "698\n",
      "698\n",
      "698\n"
     ]
    }
   ],
   "source": [
    "### validation ###\n",
    "\n",
    "## Getting data ##\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = val_df.text_corrected.values\n",
    "\n",
    "image_names = val_df.image_name.values\n",
    "\n",
    "label_humour = val_df.humour.values\n",
    "label_sarcasm = val_df.sarcasm.values\n",
    "label_offensive = val_df.offensive.values\n",
    "label_motivational = val_df.motivational.values\n",
    "label_overall_sentiment = val_df.overall_sentiment.values\n",
    "\n",
    "# Converting the labels to integers #\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "label_humour_val = []\n",
    "\n",
    "for i in label_humour:\n",
    "    if i == 'not_funny':\n",
    "        label_humour_val.append(0)\n",
    "    elif i == 'funny':\n",
    "        label_humour_val.append(1)\n",
    "    elif i == 'very_funny':\n",
    "        label_humour_val.append(2)\n",
    "    elif i == 'hilarious':\n",
    "        label_humour_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_sarcasm_val = []\n",
    "\n",
    "for i in label_sarcasm:\n",
    "    if i == 'not_sarcastic':\n",
    "        label_sarcasm_val.append(0)\n",
    "    elif i == 'general':\n",
    "        label_sarcasm_val.append(1)\n",
    "    elif i == 'twisted_meaning':\n",
    "        label_sarcasm_val.append(2)\n",
    "    elif i == 'very_twisted':\n",
    "        label_sarcasm_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_offensive_val = []\n",
    "\n",
    "for i in label_offensive:\n",
    "    if i == 'not_offensive':\n",
    "        label_offensive_val.append(0)\n",
    "    elif i == 'slight':\n",
    "        label_offensive_val.append(1)\n",
    "    elif i == 'very_offensive':\n",
    "        label_offensive_val.append(2)\n",
    "    elif i == 'hateful_offensive':\n",
    "        label_offensive_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_motivational_val = []\n",
    "\n",
    "for i in label_motivational:\n",
    "    if i == 'not_motivational':\n",
    "        label_motivational_val.append(0)\n",
    "    elif i == 'motivational':\n",
    "        label_motivational_val.append(1)\n",
    "        \n",
    "\n",
    "label_overall_sentiment_val = []\n",
    "\n",
    "for i in label_overall_sentiment:\n",
    "    if i == 'very_negative':\n",
    "        label_overall_sentiment_val.append(0)\n",
    "    elif i == 'negative':\n",
    "        label_overall_sentiment_val.append(1)\n",
    "    elif i == 'neutral':\n",
    "        label_overall_sentiment_val.append(2)\n",
    "    elif i == 'positive':\n",
    "        label_overall_sentiment_val.append(3)\n",
    "    elif i == 'very_positive':\n",
    "        label_overall_sentiment_val.append(4)\n",
    "        \n",
    "        \n",
    "## Converting to numpy array ##\n",
    "label_humour_val = np.array(label_humour_val)\n",
    "label_sarcasm_val = np.array(label_sarcasm_val)\n",
    "label_offensive_val = np.array(label_offensive_val)\n",
    "label_motivational_val = np.array(label_motivational_val)\n",
    "label_overall_sentiment_val = np.array(label_overall_sentiment_val)\n",
    "\n",
    "## checking the dimensions -sanity check ##\n",
    "print(len(label_humour_val))\n",
    "print(len(label_sarcasm_val))\n",
    "print(len(label_offensive_val))\n",
    "print(len(label_motivational_val))\n",
    "print(len(label_overall_sentiment_val))\n",
    "\n",
    "\n",
    "## Preprocessing the image ##\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "preprocessed_image = []\n",
    "\n",
    "for img in image_names:\n",
    "    \n",
    "    img_tensor = cv2.imread('./memotion_dataset_7k/images/'+img)\n",
    "    img_tensor = cv2.resize(img_tensor,(224,224)) ## commonly used dimensison - 224\n",
    "    img_tensor = cv2.cvtColor(img_tensor, cv2.COLOR_BGR2RGB) ## format used by pytorch\n",
    "    # converting the type of pixel to float 32\n",
    "    img_tensor = img_tensor.astype('float32')\n",
    "    img_tensor = transform(img_tensor)\n",
    "    preprocessed_image.append(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "## Val dataset and dataloader ##\n",
    "\n",
    "val = Dataset(val_df)\n",
    "val_dataloader = torch.utils.data.DataLoader(val, batch_size=4)\n",
    "\n",
    "print(len(val))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699\n",
      "699\n",
      "699\n",
      "699\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "### Testing ###\n",
    "\n",
    "## Getting data ##\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = test_df.text_corrected.values\n",
    "\n",
    "image_names = test_df.image_name.values\n",
    "\n",
    "label_humour = test_df.humour.values\n",
    "label_sarcasm = test_df.sarcasm.values\n",
    "label_offensive = test_df.offensive.values\n",
    "label_motivational = test_df.motivational.values\n",
    "label_overall_sentiment = test_df.overall_sentiment.values\n",
    "\n",
    "# Converting the labels to integers #\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "label_humour_val = []\n",
    "\n",
    "for i in label_humour:\n",
    "    if i == 'not_funny':\n",
    "        label_humour_val.append(0)\n",
    "    elif i == 'funny':\n",
    "        label_humour_val.append(1)\n",
    "    elif i == 'very_funny':\n",
    "        label_humour_val.append(2)\n",
    "    elif i == 'hilarious':\n",
    "        label_humour_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_sarcasm_val = []\n",
    "\n",
    "for i in label_sarcasm:\n",
    "    if i == 'not_sarcastic':\n",
    "        label_sarcasm_val.append(0)\n",
    "    elif i == 'general':\n",
    "        label_sarcasm_val.append(1)\n",
    "    elif i == 'twisted_meaning':\n",
    "        label_sarcasm_val.append(2)\n",
    "    elif i == 'very_twisted':\n",
    "        label_sarcasm_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_offensive_val = []\n",
    "\n",
    "for i in label_offensive:\n",
    "    if i == 'not_offensive':\n",
    "        label_offensive_val.append(0)\n",
    "    elif i == 'slight':\n",
    "        label_offensive_val.append(1)\n",
    "    elif i == 'very_offensive':\n",
    "        label_offensive_val.append(2)\n",
    "    elif i == 'hateful_offensive':\n",
    "        label_offensive_val.append(3)\n",
    "        \n",
    "        \n",
    "        \n",
    "label_motivational_val = []\n",
    "\n",
    "for i in label_motivational:\n",
    "    if i == 'not_motivational':\n",
    "        label_motivational_val.append(0)\n",
    "    elif i == 'motivational':\n",
    "        label_motivational_val.append(1)\n",
    "        \n",
    "\n",
    "label_overall_sentiment_val = []\n",
    "\n",
    "for i in label_overall_sentiment:\n",
    "    if i == 'very_negative':\n",
    "        label_overall_sentiment_val.append(0)\n",
    "    elif i == 'negative':\n",
    "        label_overall_sentiment_val.append(1)\n",
    "    elif i == 'neutral':\n",
    "        label_overall_sentiment_val.append(2)\n",
    "    elif i == 'positive':\n",
    "        label_overall_sentiment_val.append(3)\n",
    "    elif i == 'very_positive':\n",
    "        label_overall_sentiment_val.append(4)\n",
    "        \n",
    "        \n",
    "## Converting to numpy array ##\n",
    "label_humour_val = np.array(label_humour_val)\n",
    "label_sarcasm_val = np.array(label_sarcasm_val)\n",
    "label_offensive_val = np.array(label_offensive_val)\n",
    "label_motivational_val = np.array(label_motivational_val)\n",
    "label_overall_sentiment_val = np.array(label_overall_sentiment_val)\n",
    "\n",
    "## checking the dimensions -sanity check ##\n",
    "print(len(label_humour_val))\n",
    "print(len(label_sarcasm_val))\n",
    "print(len(label_offensive_val))\n",
    "print(len(label_motivational_val))\n",
    "print(len(label_overall_sentiment_val))\n",
    "\n",
    "\n",
    "## Preprocessing the image ##\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "preprocessed_image = []\n",
    "\n",
    "for img in image_names:\n",
    "    \n",
    "    img_tensor = cv2.imread('./memotion_dataset_7k/images/'+img)\n",
    "    img_tensor = cv2.resize(img_tensor,(224,224)) ## commonly used dimensison - 224\n",
    "    img_tensor = cv2.cvtColor(img_tensor, cv2.COLOR_BGR2RGB) ## format used by pytorch\n",
    "    # converting the type of pixel to float 32\n",
    "    img_tensor = img_tensor.astype('float32')\n",
    "    img_tensor = transform(img_tensor)\n",
    "    preprocessed_image.append(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "## Test dataset and dataloader ##\n",
    "\n",
    "test = Dataset(test_df)\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=4)\n",
    "\n",
    "print(len(val))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "# train = Dataset(df_train)\n",
    "# val = Dataset(df_val)\n",
    "# test = Dataset(df_test)\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\n",
    "# val_dataloader = torch.utils.data.DataLoader(val, batch_size=4)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test, batch_size=4) #batch size = 4\n",
    "\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#lr = 1e-3\n",
    "#optimizer = Adam(model.parameters(), lr= 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[[  101,  2115,  4299,  ...,     0,     0,     0]],\n",
       "  \n",
       "          [[  101,  9413, 29038,  ...,     0,     0,     0]],\n",
       "  \n",
       "          [[  101,  4152,  3140,  ...,     0,     0,     0]],\n",
       "  \n",
       "          [[  101,  2043,  2619,  ...,     0,     0,     0]]]),\n",
       "  'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 0, 0, 0]]])},\n",
       " tensor([[[[163., 163., 153.,  ..., 135., 111., 109.],\n",
       "           [169., 167., 169.,  ..., 143., 143., 145.],\n",
       "           [213., 203., 181.,  ..., 123., 143., 155.],\n",
       "           ...,\n",
       "           [357., 361., 377.,  ..., 203., 213., 193.],\n",
       "           [ 95., 101., 111.,  ..., 213., 215., 243.],\n",
       "           [ 47.,  55.,  49.,  ..., 201., 233., 237.]],\n",
       " \n",
       "          [[187., 183., 173.,  ..., 125.,  95.,  93.],\n",
       "           [193., 187., 189.,  ..., 131., 127., 129.],\n",
       "           [237., 223., 201.,  ..., 113., 127., 139.],\n",
       "           ...,\n",
       "           [355., 359., 375.,  ..., 193., 205., 183.],\n",
       "           [103., 109., 119.,  ..., 203., 205., 233.],\n",
       "           [ 53.,  57.,  53.,  ..., 199., 231., 237.]],\n",
       " \n",
       "          [[187., 185., 179.,  ..., 117.,  89.,  87.],\n",
       "           [193., 191., 193.,  ..., 125., 121., 123.],\n",
       "           [237., 225., 205.,  ..., 105., 121., 133.],\n",
       "           ...,\n",
       "           [371., 377., 391.,  ..., 201., 213., 191.],\n",
       "           [121., 127., 137.,  ..., 211., 213., 241.],\n",
       "           [ 61.,  67.,  63.,  ..., 203., 235., 241.]]],\n",
       " \n",
       " \n",
       "         [[[247., 263., 221.,  ..., 507., 501., 499.],\n",
       "           [229., 213., 187.,  ..., 503., 503., 501.],\n",
       "           [195., 173., 171.,  ..., 499., 503., 505.],\n",
       "           ...,\n",
       "           [213., 213., 215.,  ..., 371., 411., 293.],\n",
       "           [213., 215., 217.,  ..., 217., 257., 217.],\n",
       "           [217., 217., 219.,  ..., 201., 177., 129.]],\n",
       " \n",
       "          [[285., 305., 265.,  ..., 509., 509., 509.],\n",
       "           [265., 251., 229.,  ..., 509., 509., 509.],\n",
       "           [231., 209., 209.,  ..., 509., 509., 507.],\n",
       "           ...,\n",
       "           [213., 213., 215.,  ..., 379., 415., 297.],\n",
       "           [213., 215., 217.,  ..., 229., 263., 221.],\n",
       "           [217., 217., 219.,  ..., 213., 185., 137.]],\n",
       " \n",
       "          [[277., 269., 193.,  ..., 501., 507., 507.],\n",
       "           [249., 215., 163.,  ..., 505., 505., 505.],\n",
       "           [203., 171., 155.,  ..., 507., 503., 501.],\n",
       "           ...,\n",
       "           [213., 213., 215.,  ..., 377., 413., 295.],\n",
       "           [213., 215., 217.,  ..., 225., 261., 219.],\n",
       "           [217., 217., 219.,  ..., 209., 183., 135.]]],\n",
       " \n",
       " \n",
       "         [[[ 37.,  43.,  43.,  ...,  35.,  31.,  27.],\n",
       "           [ 39.,  37.,  37.,  ...,  29.,  27.,  25.],\n",
       "           [ 39.,  39.,  43.,  ...,  51.,  53.,  47.],\n",
       "           ...,\n",
       "           [387., 391., 391.,  ..., 301., 303., 123.],\n",
       "           [385., 383., 391.,  ..., 307., 295., 199.],\n",
       "           [387., 379., 383.,  ..., 265., 163., 157.]],\n",
       " \n",
       "          [[ 39.,  45.,  43.,  ...,  37.,  33.,  31.],\n",
       "           [ 41.,  39.,  39.,  ...,  29.,  29.,  27.],\n",
       "           [ 41.,  41.,  45.,  ...,  53.,  53.,  47.],\n",
       "           ...,\n",
       "           [407., 411., 403.,  ..., 215., 219.,  43.],\n",
       "           [405., 405., 413.,  ..., 215., 207., 113.],\n",
       "           [401., 399., 403.,  ..., 171.,  73.,  71.]],\n",
       " \n",
       "          [[ 29.,  35.,  39.,  ...,  41.,  35.,  31.],\n",
       "           [ 31.,  29.,  29.,  ...,  25.,  19.,  19.],\n",
       "           [ 31.,  31.,  35.,  ...,  41.,  37.,  29.],\n",
       "           ...,\n",
       "           [457., 455., 447.,  ..., 181., 189.,  29.],\n",
       "           [455., 453., 457.,  ..., 183., 181.,  97.],\n",
       "           [457., 449., 453.,  ..., 143.,  45.,  45.]]],\n",
       " \n",
       " \n",
       "         [[[499., 493., 493.,  ..., 505., 509., 509.],\n",
       "           [499., 495., 499.,  ..., 501., 507., 509.],\n",
       "           [493., 499., 471.,  ..., 507., 507., 509.],\n",
       "           ...,\n",
       "           [497., 487., 493.,  ..., 493., 499., 503.],\n",
       "           [499., 499., 499.,  ..., 503., 503., 505.],\n",
       "           [499., 499., 501.,  ..., 503., 505., 507.]],\n",
       " \n",
       "          [[507., 501., 497.,  ..., 505., 509., 509.],\n",
       "           [507., 503., 505.,  ..., 501., 507., 509.],\n",
       "           [499., 507., 483.,  ..., 507., 507., 509.],\n",
       "           ...,\n",
       "           [505., 495., 499.,  ..., 499., 505., 505.],\n",
       "           [509., 509., 507.,  ..., 507., 505., 505.],\n",
       "           [509., 509., 507.,  ..., 507., 507., 505.]],\n",
       " \n",
       "          [[507., 499., 493.,  ..., 505., 509., 509.],\n",
       "           [497., 495., 499.,  ..., 501., 507., 509.],\n",
       "           [483., 493., 471.,  ..., 507., 507., 509.],\n",
       "           ...,\n",
       "           [495., 485., 489.,  ..., 489., 495., 499.],\n",
       "           [501., 501., 499.,  ..., 497., 495., 501.],\n",
       "           [501., 501., 501.,  ..., 497., 497., 501.]]]]),\n",
       " ('image_2167.jpg', 'image_5879.png', 'image_3921.jpg', 'image_6208.jpg'),\n",
       " tensor([4, 2, 3, 1], dtype=torch.int32),\n",
       " tensor([2, 3, 1, 3], dtype=torch.int32),\n",
       " tensor([2, 2, 0, 0], dtype=torch.int32),\n",
       " tensor([1, 2, 0, 0], dtype=torch.int32),\n",
       " tensor([1, 0, 0, 0], dtype=torch.int32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sanity check: dataloaders ##\n",
    "\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, prev_val_loss, validation_loss):\n",
    "        if (validation_loss - prev_val_loss) >= self.min_delta:\n",
    "            self.counter +=1\n",
    "            print('Early stopping counter value: ', self.counter)\n",
    "            if self.counter >= self.patience:  \n",
    "                self.early_stop = True\n",
    "                \n",
    "## Ref: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment is conducted in three different settings:\n",
    "\n",
    "    1) Text-only analysis\n",
    "\n",
    "    2) Image-only analysis\n",
    "\n",
    "    3) Multimodal analysis (Text+image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-only analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class DistilBertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(DistilBertClassifier, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.classifier_humour = nn.Linear(768, 4)\n",
    "        self.classifier_sarcasm = nn.Linear(768, 4)\n",
    "        self.classifier_offensive = nn.Linear(768, 4)\n",
    "        self.classifier_motivational = nn.Linear(768, 2)\n",
    "        self.classifier_sentiment = nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        distilbert_output = self.distilbert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        \n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        \n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.relu(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        \n",
    "        logits_humour = self.classifier_humour(pooled_output)  # (bs, num_labels)\n",
    "        logits_sarcasm = self.classifier_sarcasm(pooled_output)\n",
    "        logits_offensive = self.classifier_offensive(pooled_output)\n",
    "        logits_motivational = self.classifier_motivational(pooled_output)\n",
    "        logits_sentiment = self.classifier_sentiment(pooled_output)\n",
    "\n",
    "        return logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertClassifier().to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertClassifier(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (classifier_humour): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (classifier_sarcasm): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (classifier_offensive): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (classifier_motivational): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (classifier_sentiment): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, epochs, criterion, optimizer, device):\n",
    "    \n",
    "    prev_val_loss = 0\n",
    "    \n",
    "    ## Tracking the accuracy and losses for plotting ##\n",
    "    \n",
    "    train_loss_tracker = []\n",
    "    val_loss_tracker = []\n",
    "    \n",
    "    train_value_tracker = {'humour':[], 'sarcasm': [], 'off':[], 'moti':[], 'senti':[]}\n",
    "    val_value_tracker = {'humour':[], 'sarcasm': [], 'off':[], 'moti':[], 'senti':[]}\n",
    "            \n",
    "    early_stopping = EarlyStopping()\n",
    "    \n",
    "    #model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "        train_items_total = 0\n",
    "        val_items_total = 0\n",
    "    \n",
    "        total_acc_train_humour = 0\n",
    "        total_acc_train_sarcasm = 0\n",
    "        total_acc_train_off = 0\n",
    "        total_acc_train_moti = 0\n",
    "        total_acc_train_senti = 0\n",
    "\n",
    "        total_acc_val_humour = 0\n",
    "        total_acc_val_sarcasm = 0\n",
    "        total_acc_val_off = 0\n",
    "        total_acc_val_moti = 0\n",
    "        total_acc_val_senti = 0\n",
    "\n",
    "        total_loss_val = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, _, _, train_label_sentiment, train_label_humour, train_label_sarcasm, train_label_offensive, train_label_motivational in tqdm(train_dataloader):\n",
    "\n",
    "            train_label_sentiment = train_label_sentiment.to(device)\n",
    "            train_label_humour = train_label_humour.to(device)\n",
    "            train_label_sarcasm = train_label_sarcasm.to(device)\n",
    "            train_label_offensive = train_label_offensive.to(device)\n",
    "            train_label_motivational = train_label_motivational.to(device)\n",
    "\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            model=model.to(device) ## don't comment this line -- very important\n",
    "\n",
    "            logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(input_id, mask)\n",
    "            #print(output.shape)\n",
    "            #print(train_label.long().shape)\n",
    "            ##print(train_label.reshape(2,1).long().shape)\n",
    "\n",
    "            batch_loss = criterion(logits_humour, train_label_humour.long()) + criterion(logits_sarcasm, train_label_sarcasm.long()) + criterion(logits_offensive, train_label_offensive.long()) + criterion(logits_motivational, train_label_motivational.long()) + criterion(logits_sentiment, train_label_sentiment.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc_humour = (logits_humour.argmax(dim=1) == train_label_humour).sum().item()\n",
    "            total_acc_train_humour += acc_humour\n",
    "#             print(total_acc_train_humour)\n",
    "#             print(acc_humour)\n",
    "\n",
    "            acc_sarcasm = (logits_sarcasm.argmax(dim=1) == train_label_sarcasm).sum().item()\n",
    "            total_acc_train_sarcasm += acc_sarcasm\n",
    "\n",
    "            acc_off = (logits_offensive.argmax(dim=1) == train_label_offensive).sum().item()\n",
    "            total_acc_train_off += acc_off\n",
    "\n",
    "            acc_moti = (logits_motivational.argmax(dim=1) == train_label_motivational).sum().item()\n",
    "            total_acc_train_moti += acc_moti\n",
    "\n",
    "            acc_senti = (logits_sentiment.argmax(dim=1) == train_label_sentiment).sum().item()\n",
    "            total_acc_train_senti += acc_senti\n",
    "            \n",
    "            ## number of training items in each batch ##\n",
    "            train_items_total += train_label_humour.shape[0]\n",
    "            #print(train_label_humour.shape[0])\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ## Eval ##\n",
    "\n",
    "        model.eval()\n",
    "        for val_input, _, _, val_label_sentiment, val_label_humour, val_label_sarcasm, val_label_offensive, val_label_motivational in val_dataloader:\n",
    "\n",
    "            val_label_sentiment = val_label_sentiment.to(device)\n",
    "            val_label_humour = val_label_humour.to(device)\n",
    "            val_label_sarcasm = val_label_sarcasm.to(device)\n",
    "            val_label_offensive = val_label_offensive.to(device)\n",
    "            val_label_motivational = val_label_motivational.to(device)\n",
    "\n",
    "            mask = val_input['attention_mask'].to(device)\n",
    "            input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(input_id, mask)\n",
    "\n",
    "            batch_loss = criterion(logits_humour, val_label_humour.long()) + criterion(logits_sarcasm, val_label_sarcasm.long()) + criterion(logits_offensive, val_label_offensive.long()) + criterion(logits_motivational, val_label_motivational.long()) + criterion(logits_sentiment, val_label_sentiment.long())\n",
    "            total_loss_val += batch_loss.item()\n",
    "\n",
    "            acc_humour = (logits_humour.argmax(dim=1) == val_label_humour).sum().item()\n",
    "            total_acc_val_humour += acc_humour\n",
    "            #print(total_acc_val_humour)\n",
    "\n",
    "            acc_sarcasm = (logits_sarcasm.argmax(dim=1) == val_label_sarcasm).sum().item()\n",
    "            total_acc_val_sarcasm += acc_sarcasm\n",
    "\n",
    "            acc_off = (logits_offensive.argmax(dim=1) == val_label_offensive).sum().item()\n",
    "            total_acc_val_off += acc_off\n",
    "\n",
    "            acc_moti = (logits_motivational.argmax(dim=1) == val_label_motivational).sum().item()\n",
    "            total_acc_val_moti += acc_moti\n",
    "\n",
    "            acc_senti = (logits_sentiment.argmax(dim=1) == val_label_sentiment).sum().item()\n",
    "            total_acc_val_senti += acc_senti\n",
    "            \n",
    "            ## number of validation items in each batch ##\n",
    "            val_items_total += val_label_humour.shape[0]\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} \\n | Train Loss: {total_loss_train / train_items_total: .3f} \\n\\n \\\n",
    "            | Train Accuracy (Humour): {total_acc_train_humour / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Sarcasm): {total_acc_train_sarcasm / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Offensive): {total_acc_train_off / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Motivational): {total_acc_train_moti / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Overall Sentiment): {total_acc_train_senti / train_items_total: .3f} \\n \\n \\n \\\n",
    "            | Val Loss: {total_loss_val / val_items_total: .3f} \\n\\n \\\n",
    "            | Val Accuracy (Humour): {total_acc_val_humour / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Sarcasm): {total_acc_val_sarcasm / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Offensive): {total_acc_val_off / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Motivational): {total_acc_val_moti / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Overall Sentiment): {total_acc_val_senti / val_items_total: .3f} \\n\\n\\n')\n",
    "\n",
    "        ## Adding to the list for plotting ##\n",
    "        train_loss_tracker.append(total_loss_train / train_items_total) \n",
    "        val_loss_tracker.append(total_loss_val / val_items_total)\n",
    "\n",
    "        train_value_tracker['humour'].append(total_acc_train_humour / train_items_total)\n",
    "        train_value_tracker['sarcasm'].append(total_acc_train_sarcasm / train_items_total)\n",
    "        train_value_tracker['off'].append(total_acc_train_off / train_items_total)\n",
    "        train_value_tracker['moti'].append(total_acc_train_moti / train_items_total)\n",
    "        train_value_tracker['senti'].append(total_acc_train_senti / train_items_total)\n",
    "\n",
    "        val_value_tracker['humour'].append(total_acc_val_humour / val_items_total)\n",
    "        val_value_tracker['sarcasm'].append(total_acc_val_sarcasm / val_items_total)\n",
    "        val_value_tracker['off'].append(total_acc_val_off / val_items_total)\n",
    "        val_value_tracker['moti'].append(total_acc_val_moti / val_items_total)\n",
    "        val_value_tracker['senti'].append(total_acc_val_senti / val_items_total)\n",
    "\n",
    "\n",
    "        ## Early stopping ##\n",
    "\n",
    "        if epoch_num != 0:\n",
    "            early_stopping(prev_val_loss, total_loss_val / val_items_total)\n",
    "            #prev_val_loss = total_loss_val / val_items_total\n",
    "            \n",
    "            current_val_loss = total_loss_val / val_items_total ## updated\n",
    "            if current_val_loss < prev_val_loss: # if current value is less than the previous value\n",
    "                prev_val_loss = current_val_loss\n",
    "\n",
    "        elif epoch_num == 0:\n",
    "            prev_val_loss = total_loss_val / val_items_total\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Training stopped at epoch:\", epoch_num+1)\n",
    "            break\n",
    "                    \n",
    "    return train_loss_tracker, val_loss_tracker, train_value_tracker, val_value_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model = DistilBertClassifier()\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr= lr)\n",
    "\n",
    "train_loss_tracker, val_loss_tracker, train_value_tracker, val_value_tracker = train(model, train_dataloader, val_dataloader, epochs, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output: ##\n",
    "\n",
    "100%|██████████████████████████████████████████████████████████████████████████████| 1397/1397 [05:35<00:00,  4.17it/s]\n",
    "\n",
    "Epochs: 9 \n",
    " | Train Loss:  1.395 \n",
    " | Val Loss:  1.379 \n",
    "\n",
    "\n",
    "\n",
    "Early stopping counter value:  3\n",
    "\n",
    "Training stopped at epoch: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters :  66968083\n"
     ]
    }
   ],
   "source": [
    "## Counting the number of model parameters ##\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of parameters : ', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test(model, test_dataloader, criterion, optimizer, device):\n",
    "    total_acc_test_humour = 0\n",
    "    total_acc_test_sarcasm = 0\n",
    "    total_acc_test_off = 0\n",
    "    total_acc_test_moti = 0\n",
    "    total_acc_test_senti = 0\n",
    "\n",
    "    total_loss_test = 0\n",
    "    \n",
    "    ## total test items in a dataloader ##\n",
    "    test_items_total = 0\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    img_lst =[]\n",
    "    predicted_labels_lst_hum=[]\n",
    "    orig_labels_lst_hum=[]\n",
    "    \n",
    "    predicted_labels_lst_sar=[]\n",
    "    orig_labels_lst_sar=[]\n",
    "    \n",
    "    predicted_labels_lst_moti=[]\n",
    "    orig_labels_lst_moti=[]\n",
    "    \n",
    "    predicted_labels_lst_off=[]\n",
    "    orig_labels_lst_off=[]\n",
    "    \n",
    "    predicted_labels_lst_senti=[]\n",
    "    orig_labels_lst_senti=[]\n",
    "    \n",
    "    for test_input, img_input, img_names, test_label_sentiment, test_label_humour, test_label_sarcasm, test_label_offensive, test_label_motivational in test_dataloader:\n",
    "        test_label_sentiment = test_label_sentiment.to(device)\n",
    "        test_label_humour = test_label_humour.to(device)\n",
    "        test_label_sarcasm = test_label_sarcasm.to(device)\n",
    "        test_label_offensive = test_label_offensive.to(device)\n",
    "        test_label_motivational = test_label_motivational.to(device)\n",
    "\n",
    "        mask = test_input['attention_mask'].to(device)\n",
    "        input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(input_id, mask)\n",
    "            \n",
    "        img_lst.extend(list(img_names))\n",
    "#         print(test_label_sentiment.tolist())\n",
    "#         print(logits_humour.argmax(dim=1).tolist())\n",
    "#         print(img_names)\n",
    "        \n",
    "        predicted_labels_lst_hum.extend(logits_humour.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_hum.extend(test_label_humour.tolist())\n",
    "        \n",
    "        predicted_labels_lst_sar.extend(logits_sarcasm.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_sar.extend(test_label_sarcasm.tolist())\n",
    "\n",
    "        predicted_labels_lst_moti.extend(logits_motivational.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_moti.extend(test_label_motivational.tolist())\n",
    "\n",
    "        predicted_labels_lst_off.extend(logits_offensive.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_off.extend(test_label_offensive.tolist())\n",
    "\n",
    "        predicted_labels_lst_senti.extend(logits_sentiment.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_senti.extend(test_label_sentiment.tolist())\n",
    "        \n",
    "        batch_loss = criterion(logits_humour, test_label_humour.long()) + criterion(logits_sarcasm, test_label_sarcasm.long()) + criterion(logits_offensive, test_label_offensive.long()) + criterion(logits_motivational, test_label_motivational.long()) + criterion(logits_sentiment, test_label_sentiment.long())\n",
    "        total_loss_test += batch_loss.item()\n",
    "\n",
    "        acc_humour = (logits_humour.argmax(dim=1) == test_label_humour).sum().item()\n",
    "        total_acc_test_humour += acc_humour\n",
    "\n",
    "        acc_sarcasm = (logits_sarcasm.argmax(dim=1) == test_label_sarcasm).sum().item()\n",
    "        total_acc_test_sarcasm += acc_sarcasm\n",
    "\n",
    "        acc_off = (logits_offensive.argmax(dim=1) == test_label_offensive).sum().item()\n",
    "        total_acc_test_off += acc_off\n",
    "\n",
    "        acc_moti = (logits_motivational.argmax(dim=1) == test_label_motivational).sum().item()\n",
    "        total_acc_test_moti += acc_moti\n",
    "\n",
    "        acc_senti = (logits_sentiment.argmax(dim=1) == test_label_sentiment).sum().item()\n",
    "        total_acc_test_senti += acc_senti\n",
    "        \n",
    "        ## number of test items in each batch ##\n",
    "        test_items_total += test_label_humour.shape[0]\n",
    "\n",
    "    print(\n",
    "        f'| Test Loss: {total_loss_test / test_items_total: .3f} \\n\\n \\\n",
    "        | Test Accuracy (Humour): {total_acc_test_humour / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Sarcasm): {total_acc_test_sarcasm / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Offensive): {total_acc_test_off / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Motivational): {total_acc_test_moti / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Overall Sentiment): {total_acc_test_senti / test_items_total: .3f} \\n\\n\\n')\n",
    "    \n",
    "    print('\\n\\n*********************Classification report***************\\n\\n')\n",
    "    print('Humour:')\n",
    "    print(classification_report(orig_labels_lst_hum, predicted_labels_lst_hum))\n",
    "    print('Sarcasm:')\n",
    "    print(classification_report(orig_labels_lst_sar, predicted_labels_lst_sar))\n",
    "    print('Offensive:')\n",
    "    print(classification_report(orig_labels_lst_off, predicted_labels_lst_off))\n",
    "    print('Motivational:')\n",
    "    print(classification_report(orig_labels_lst_moti, predicted_labels_lst_moti))\n",
    "    print('Overall Sentiment:')\n",
    "    print(classification_report(orig_labels_lst_senti, predicted_labels_lst_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_dataloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Output:</b> \n",
    "\n",
    "Test Accuracy (Humour): 31%\n",
    "\n",
    "Test Accuracy (Offensive): 39%\n",
    "\n",
    "Test Accuracy (Sarcasm): 47%\n",
    "\n",
    "Test Accuracy (Motivational): 64%\n",
    "\n",
    "Test Accuracy (Overall Sentiment): 45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model ###\n",
    "\n",
    "torch.save(model, './text_distilbert_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the charts - Accuracies -Train ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_hum = train_value_tracker['humour'].copy()\n",
    "y_pts_sar = train_value_tracker['sarcasm'].copy()\n",
    "y_pts_off = train_value_tracker['off'].copy()\n",
    "y_pts_moti = train_value_tracker['moti'].copy()\n",
    "y_pts_senti = train_value_tracker['senti'].copy()\n",
    "\n",
    "x_pts = [x for x in range(1,len(y_pts_hum)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_hum)\n",
    "plt.plot(x_pts,y_pts_sar)\n",
    "plt.plot(x_pts,y_pts_off)\n",
    "plt.plot(x_pts,y_pts_moti)\n",
    "plt.plot(x_pts,y_pts_senti)\n",
    "\n",
    "plt.title('Training chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.legend(['humour', 'sarcasm', 'offensive', 'motivational', 'sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the charts - Accuracies -Val ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_hum = val_value_tracker['humour'].copy()\n",
    "y_pts_sar = val_value_tracker['sarcasm'].copy()\n",
    "y_pts_off = val_value_tracker['off'].copy()\n",
    "y_pts_moti = val_value_tracker['moti'].copy()\n",
    "y_pts_senti = val_value_tracker['senti'].copy()\n",
    "\n",
    "x_pts = [x for x in range(1,len(y_pts_hum)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_hum)\n",
    "plt.plot(x_pts,y_pts_sar)\n",
    "plt.plot(x_pts,y_pts_off)\n",
    "plt.plot(x_pts,y_pts_moti)\n",
    "plt.plot(x_pts,y_pts_senti)\n",
    "\n",
    "plt.title('Training chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.legend(['humour', 'sarcasm', 'offensive', 'motivational', 'sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the charts - loss - train -Val ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_loss_train = train_loss_tracker.copy()\n",
    "y_pts_loss_val = val_loss_tracker.copy()\n",
    "x_pts = [x for x in range(1,len(y_pts_loss_val)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_loss_train)\n",
    "plt.plot(x_pts,y_pts_loss_val)\n",
    "\n",
    "plt.title('Training and Validation Loss chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-only analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResnetClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.resnet50.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 2048),\n",
    "               nn.ReLU(inplace=True))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.classifier_humour = nn.Linear(2048, 4)\n",
    "        self.classifier_sarcasm = nn.Linear(2048, 4)\n",
    "        self.classifier_offensive = nn.Linear(2048, 4)\n",
    "        self.classifier_motivational = nn.Linear(2048, 2)\n",
    "        self.classifier_sentiment = nn.Linear(2048, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x) # (bs, dim) \n",
    "        x = self.relu(x)  # (bs, dim)\n",
    "        x = self.dropout(x)  # (bs, dim)\n",
    "        \n",
    "        logits_humour = self.classifier_humour(x)  # (bs, num_labels)\n",
    "        logits_sarcasm = self.classifier_sarcasm(x)\n",
    "        logits_offensive = self.classifier_offensive(x)\n",
    "        logits_motivational = self.classifier_motivational(x)\n",
    "        logits_sentiment = self.classifier_sentiment(x)\n",
    "\n",
    "        return logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetClassifier().to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetClassifier(\n",
       "  (resnet50): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (classifier_humour): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  (classifier_sarcasm): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  (classifier_offensive): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  (classifier_motivational): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  (classifier_sentiment): Linear(in_features=2048, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters :  27743315\n"
     ]
    }
   ],
   "source": [
    "## Counting the number of model parameters ##\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of parameters : ', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, epochs, criterion, optimizer, device):\n",
    "    \n",
    "    prev_val_loss = 0\n",
    "    \n",
    "    ## Tracking the accuracy and losses for plotting ##\n",
    "    \n",
    "    train_loss_tracker = []\n",
    "    val_loss_tracker = []\n",
    "    \n",
    "    train_value_tracker = {'humour':[], 'sarcasm': [], 'off':[], 'moti':[], 'senti':[]}\n",
    "    val_value_tracker = {'humour':[], 'sarcasm': [], 'off':[], 'moti':[], 'senti':[]}\n",
    "            \n",
    "    early_stopping = EarlyStopping()\n",
    "    \n",
    "    #model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "        train_items_total = 0\n",
    "        val_items_total = 0\n",
    "    \n",
    "        total_acc_train_humour = 0\n",
    "        total_acc_train_sarcasm = 0\n",
    "        total_acc_train_off = 0\n",
    "        total_acc_train_moti = 0\n",
    "        total_acc_train_senti = 0\n",
    "\n",
    "        total_acc_val_humour = 0\n",
    "        total_acc_val_sarcasm = 0\n",
    "        total_acc_val_off = 0\n",
    "        total_acc_val_moti = 0\n",
    "        total_acc_val_senti = 0\n",
    "\n",
    "        total_loss_val = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for _, img_input, _, train_label_sentiment, train_label_humour, train_label_sarcasm, train_label_offensive, train_label_motivational in tqdm(train_dataloader):\n",
    "\n",
    "            train_label_sentiment = train_label_sentiment.to(device)\n",
    "            train_label_humour = train_label_humour.to(device)\n",
    "            train_label_sarcasm = train_label_sarcasm.to(device)\n",
    "            train_label_offensive = train_label_offensive.to(device)\n",
    "            train_label_motivational = train_label_motivational.to(device)\n",
    "\n",
    "            img_input = img_input.to(device)\n",
    "\n",
    "            model=model.to(device) ## don't comment this line -- very important\n",
    "\n",
    "            logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(img_input)\n",
    "            #print(output.shape)\n",
    "            #print(train_label.long().shape)\n",
    "            ##print(train_label.reshape(2,1).long().shape)\n",
    "\n",
    "            batch_loss = criterion(logits_humour, train_label_humour.long()) + criterion(logits_sarcasm, train_label_sarcasm.long()) + criterion(logits_offensive, train_label_offensive.long()) + criterion(logits_motivational, train_label_motivational.long()) + criterion(logits_sentiment, train_label_sentiment.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc_humour = (logits_humour.argmax(dim=1) == train_label_humour).sum().item()\n",
    "            total_acc_train_humour += acc_humour\n",
    "#             print(total_acc_train_humour)\n",
    "#             print(acc_humour)\n",
    "\n",
    "            acc_sarcasm = (logits_sarcasm.argmax(dim=1) == train_label_sarcasm).sum().item()\n",
    "            total_acc_train_sarcasm += acc_sarcasm\n",
    "\n",
    "            acc_off = (logits_offensive.argmax(dim=1) == train_label_offensive).sum().item()\n",
    "            total_acc_train_off += acc_off\n",
    "\n",
    "            acc_moti = (logits_motivational.argmax(dim=1) == train_label_motivational).sum().item()\n",
    "            total_acc_train_moti += acc_moti\n",
    "\n",
    "            acc_senti = (logits_sentiment.argmax(dim=1) == train_label_sentiment).sum().item()\n",
    "            total_acc_train_senti += acc_senti\n",
    "            \n",
    "            ## number of training items in each batch ##\n",
    "            train_items_total += train_label_humour.shape[0]\n",
    "            #print(train_label_humour.shape[0])\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ## Eval ##\n",
    "\n",
    "        model.eval()\n",
    "        for _, img_input, _, val_label_sentiment, val_label_humour, val_label_sarcasm, val_label_offensive, val_label_motivational in val_dataloader:\n",
    "            val_label_sentiment = val_label_sentiment.to(device)\n",
    "            val_label_humour = val_label_humour.to(device)\n",
    "            val_label_sarcasm = val_label_sarcasm.to(device)\n",
    "            val_label_offensive = val_label_offensive.to(device)\n",
    "            val_label_motivational = val_label_motivational.to(device)\n",
    "\n",
    "            img_input = img_input.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(img_input)\n",
    "\n",
    "            batch_loss = criterion(logits_humour, val_label_humour.long()) + criterion(logits_sarcasm, val_label_sarcasm.long()) + criterion(logits_offensive, val_label_offensive.long()) + criterion(logits_motivational, val_label_motivational.long()) + criterion(logits_sentiment, val_label_sentiment.long())\n",
    "            total_loss_val += batch_loss.item()\n",
    "\n",
    "            acc_humour = (logits_humour.argmax(dim=1) == val_label_humour).sum().item()\n",
    "            total_acc_val_humour += acc_humour\n",
    "            #print(total_acc_val_humour)\n",
    "\n",
    "            acc_sarcasm = (logits_sarcasm.argmax(dim=1) == val_label_sarcasm).sum().item()\n",
    "            total_acc_val_sarcasm += acc_sarcasm\n",
    "\n",
    "            acc_off = (logits_offensive.argmax(dim=1) == val_label_offensive).sum().item()\n",
    "            total_acc_val_off += acc_off\n",
    "\n",
    "            acc_moti = (logits_motivational.argmax(dim=1) == val_label_motivational).sum().item()\n",
    "            total_acc_val_moti += acc_moti\n",
    "\n",
    "            acc_senti = (logits_sentiment.argmax(dim=1) == val_label_sentiment).sum().item()\n",
    "            total_acc_val_senti += acc_senti\n",
    "            \n",
    "            ## number of validation items in each batch ##\n",
    "            val_items_total += val_label_humour.shape[0]\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} \\n | Train Loss: {total_loss_train / train_items_total: .3f} \\n\\n \\\n",
    "            | Train Accuracy (Humour): {total_acc_train_humour / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Sarcasm): {total_acc_train_sarcasm / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Offensive): {total_acc_train_off / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Motivational): {total_acc_train_moti / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Overall Sentiment): {total_acc_train_senti / train_items_total: .3f} \\n \\n \\n \\\n",
    "            | Val Loss: {total_loss_val / val_items_total: .3f} \\n\\n \\\n",
    "            | Val Accuracy (Humour): {total_acc_val_humour / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Sarcasm): {total_acc_val_sarcasm / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Offensive): {total_acc_val_off / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Motivational): {total_acc_val_moti / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Overall Sentiment): {total_acc_val_senti / val_items_total: .3f} \\n\\n\\n')\n",
    "\n",
    "        ## Adding to the list for plotting ##\n",
    "        train_loss_tracker.append(total_loss_train / train_items_total) \n",
    "        val_loss_tracker.append(total_loss_val / val_items_total)\n",
    "\n",
    "        train_value_tracker['humour'].append(total_acc_train_humour / train_items_total)\n",
    "        train_value_tracker['sarcasm'].append(total_acc_train_sarcasm / train_items_total)\n",
    "        train_value_tracker['off'].append(total_acc_train_off / train_items_total)\n",
    "        train_value_tracker['moti'].append(total_acc_train_moti / train_items_total)\n",
    "        train_value_tracker['senti'].append(total_acc_train_senti / train_items_total)\n",
    "\n",
    "        val_value_tracker['humour'].append(total_acc_val_humour / val_items_total)\n",
    "        val_value_tracker['sarcasm'].append(total_acc_val_sarcasm / val_items_total)\n",
    "        val_value_tracker['off'].append(total_acc_val_off / val_items_total)\n",
    "        val_value_tracker['moti'].append(total_acc_val_moti / val_items_total)\n",
    "        val_value_tracker['senti'].append(total_acc_val_senti / val_items_total)\n",
    "\n",
    "\n",
    "        ## Early stopping ##\n",
    "\n",
    "        if epoch_num != 0:\n",
    "            early_stopping(prev_val_loss, total_loss_val / val_items_total)\n",
    "            prev_val_loss = total_loss_val / val_items_total\n",
    "\n",
    "        elif epoch_num == 0:\n",
    "            prev_val_loss = total_loss_val / val_items_total\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Training stopped at epoch:\", epoch_num+1)\n",
    "            break\n",
    "                    \n",
    "    return train_loss_tracker, val_loss_tracker, train_value_tracker, val_value_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr= lr)\n",
    "\n",
    "train_loss_tracker, val_loss_tracker, train_value_tracker, val_value_tracker = train(model, train_dataloader, val_dataloader, epochs, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output: ##\n",
    "\n",
    "100%|██████████████████████████████████████████████████████████████████████████████| 1397/1397 [02:37<00:00,  8.89it/s]\n",
    "\n",
    "Epochs: 14 \n",
    " | Train Loss:  1.394 \n",
    " | Val Loss:  1.379\n",
    "\n",
    "\n",
    "\n",
    "Early stopping counter value:  5\n",
    "\n",
    "Training stopped at epoch: 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test(model, test_dataloader, criterion, optimizer, device):\n",
    "    total_acc_test_humour = 0\n",
    "    total_acc_test_sarcasm = 0\n",
    "    total_acc_test_off = 0\n",
    "    total_acc_test_moti = 0\n",
    "    total_acc_test_senti = 0\n",
    "\n",
    "    total_loss_test = 0\n",
    "    \n",
    "    ## total test items in a dataloader ##\n",
    "    test_items_total = 0\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    img_lst =[]\n",
    "    predicted_labels_lst_hum=[]\n",
    "    orig_labels_lst_hum=[]\n",
    "    \n",
    "    predicted_labels_lst_sar=[]\n",
    "    orig_labels_lst_sar=[]\n",
    "    \n",
    "    predicted_labels_lst_moti=[]\n",
    "    orig_labels_lst_moti=[]\n",
    "    \n",
    "    predicted_labels_lst_off=[]\n",
    "    orig_labels_lst_off=[]\n",
    "    \n",
    "    predicted_labels_lst_senti=[]\n",
    "    orig_labels_lst_senti=[]\n",
    "    \n",
    "    for test_input, img_input, img_names, test_label_sentiment, test_label_humour, test_label_sarcasm, test_label_offensive, test_label_motivational in test_dataloader:\n",
    "        test_label_sentiment = test_label_sentiment.to(device)\n",
    "        test_label_humour = test_label_humour.to(device)\n",
    "        test_label_sarcasm = test_label_sarcasm.to(device)\n",
    "        test_label_offensive = test_label_offensive.to(device)\n",
    "        test_label_motivational = test_label_motivational.to(device)\n",
    "\n",
    "        img_input = img_input.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(img_input)\n",
    "            \n",
    "        img_lst.extend(list(img_names))\n",
    "#         print(test_label_sentiment.tolist())\n",
    "#         print(logits_humour.argmax(dim=1).tolist())\n",
    "#         print(img_names)\n",
    "        \n",
    "        predicted_labels_lst_hum.extend(logits_humour.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_hum.extend(test_label_humour.tolist())\n",
    "        \n",
    "        predicted_labels_lst_sar.extend(logits_sarcasm.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_sar.extend(test_label_sarcasm.tolist())\n",
    "\n",
    "        predicted_labels_lst_moti.extend(logits_motivational.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_moti.extend(test_label_motivational.tolist())\n",
    "\n",
    "        predicted_labels_lst_off.extend(logits_offensive.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_off.extend(test_label_offensive.tolist())\n",
    "\n",
    "        predicted_labels_lst_senti.extend(logits_sentiment.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_senti.extend(test_label_sentiment.tolist())\n",
    "        \n",
    "        batch_loss = criterion(logits_humour, test_label_humour.long()) + criterion(logits_sarcasm, test_label_sarcasm.long()) + criterion(logits_offensive, test_label_offensive.long()) + criterion(logits_motivational, test_label_motivational.long()) + criterion(logits_sentiment, test_label_sentiment.long())\n",
    "        total_loss_test += batch_loss.item()\n",
    "\n",
    "        acc_humour = (logits_humour.argmax(dim=1) == test_label_humour).sum().item()\n",
    "        total_acc_test_humour += acc_humour\n",
    "\n",
    "        acc_sarcasm = (logits_sarcasm.argmax(dim=1) == test_label_sarcasm).sum().item()\n",
    "        total_acc_test_sarcasm += acc_sarcasm\n",
    "\n",
    "        acc_off = (logits_offensive.argmax(dim=1) == test_label_offensive).sum().item()\n",
    "        total_acc_test_off += acc_off\n",
    "\n",
    "        acc_moti = (logits_motivational.argmax(dim=1) == test_label_motivational).sum().item()\n",
    "        total_acc_test_moti += acc_moti\n",
    "\n",
    "        acc_senti = (logits_sentiment.argmax(dim=1) == test_label_sentiment).sum().item()\n",
    "        total_acc_test_senti += acc_senti\n",
    "        \n",
    "        ## number of test items in each batch ##\n",
    "        test_items_total += test_label_humour.shape[0]\n",
    "\n",
    "    print(\n",
    "        f'| Test Loss: {total_loss_test / test_items_total: .3f} \\n\\n \\\n",
    "        | Test Accuracy (Humour): {total_acc_test_humour / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Sarcasm): {total_acc_test_sarcasm / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Offensive): {total_acc_test_off / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Motivational): {total_acc_test_moti / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Overall Sentiment): {total_acc_test_senti / test_items_total: .3f} \\n\\n\\n')\n",
    "    \n",
    "    print('\\n\\n*********************Classification report***************\\n\\n')\n",
    "    print('Humour:')\n",
    "    print(classification_report(orig_labels_lst_hum, predicted_labels_lst_hum))\n",
    "    print('Sarcasm:')\n",
    "    print(classification_report(orig_labels_lst_sar, predicted_labels_lst_sar))\n",
    "    print('Offensive:')\n",
    "    print(classification_report(orig_labels_lst_off, predicted_labels_lst_off))\n",
    "    print('Motivational:')\n",
    "    print(classification_report(orig_labels_lst_moti, predicted_labels_lst_moti))\n",
    "    print('Overall Sentiment:')\n",
    "    print(classification_report(orig_labels_lst_senti, predicted_labels_lst_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_dataloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Output:</b> \n",
    "\n",
    "Test Accuracy (Humour): 33%\n",
    "\n",
    "Test Accuracy (Offensive): 34%\n",
    "\n",
    "Test Accuracy (Sarcasm): 47%\n",
    "\n",
    "Test Accuracy (Motivational): 57%\n",
    "\n",
    "Test Accuracy (Overall Sentiment): 44%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model ###\n",
    "\n",
    "torch.save(model, './image_resnet50_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Charts ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the charts - Accuracies -Train ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_hum = train_value_tracker['humour'].copy()\n",
    "y_pts_sar = train_value_tracker['sarcasm'].copy()\n",
    "y_pts_off = train_value_tracker['off'].copy()\n",
    "y_pts_moti = train_value_tracker['moti'].copy()\n",
    "y_pts_senti = train_value_tracker['senti'].copy()\n",
    "\n",
    "x_pts = [x for x in range(1,len(y_pts_hum)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_hum)\n",
    "plt.plot(x_pts,y_pts_sar)\n",
    "plt.plot(x_pts,y_pts_off)\n",
    "plt.plot(x_pts,y_pts_moti)\n",
    "plt.plot(x_pts,y_pts_senti)\n",
    "\n",
    "plt.title('Training chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.legend(['humour', 'sarcasm', 'offensive', 'motivational', 'sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the charts - Accuracies -Val ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_hum = val_value_tracker['humour'].copy()\n",
    "y_pts_sar = val_value_tracker['sarcasm'].copy()\n",
    "y_pts_off = val_value_tracker['off'].copy()\n",
    "y_pts_moti = val_value_tracker['moti'].copy()\n",
    "y_pts_senti = val_value_tracker['senti'].copy()\n",
    "\n",
    "x_pts = [x for x in range(1,len(y_pts_hum)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_hum)\n",
    "plt.plot(x_pts,y_pts_sar)\n",
    "plt.plot(x_pts,y_pts_off)\n",
    "plt.plot(x_pts,y_pts_moti)\n",
    "plt.plot(x_pts,y_pts_senti)\n",
    "\n",
    "plt.title('Training chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.legend(['humour', 'sarcasm', 'offensive', 'motivational', 'sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the charts - loss - train -Val ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_loss_train = train_loss_tracker.copy()\n",
    "y_pts_loss_val = val_loss_tracker.copy()\n",
    "x_pts = [x for x in range(1,len(y_pts_loss_val)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_loss_train)\n",
    "plt.plot(x_pts,y_pts_loss_val)\n",
    "\n",
    "plt.title('Training and Validation Loss chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.resnet50.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 2048),\n",
    "               nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(768+2048, 1024)\n",
    "        \n",
    "        self.classifier_humour = nn.Linear(1024, 4)\n",
    "        self.classifier_sarcasm = nn.Linear(1024, 4)\n",
    "        self.classifier_offensive = nn.Linear(1024, 4)\n",
    "        self.classifier_motivational = nn.Linear(1024, 2)\n",
    "        self.classifier_sentiment = nn.Linear(1024, 5)\n",
    "\n",
    "    def forward(self, input_id, mask, img):\n",
    "\n",
    "        distilbert_output = self.distilbert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        \n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        \n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.relu(pooled_output)  # (bs, dim)\n",
    "        \n",
    "        x = self.resnet50(img) # (bs, dim) \n",
    "        x = self.relu(x)  # (bs, dim)\n",
    "        \n",
    "        combined_output =  torch.cat((pooled_output,x), -1) ## concatenating the tensors(bs, (img_dim + text_dim))\n",
    "        \n",
    "        combined_output = self.dropout(combined_output)  # (bs, dim)\n",
    "        combined_output = self.fc1(combined_output)\n",
    "        combined_output = self.relu(combined_output)\n",
    "        \n",
    "        \n",
    "        logits_humour = self.classifier_humour(combined_output)  # (bs, num_labels)\n",
    "        logits_sarcasm = self.classifier_sarcasm(combined_output)\n",
    "        logits_offensive = self.classifier_offensive(combined_output)\n",
    "        logits_motivational = self.classifier_motivational(combined_output)\n",
    "        logits_sentiment = self.classifier_sentiment(combined_output)\n",
    "\n",
    "        return logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = MultimodalClassifier().to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalClassifier(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (resnet50): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "  (classifier_humour): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  (classifier_sarcasm): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  (classifier_offensive): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  (classifier_motivational): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  (classifier_sentiment): Linear(in_features=1024, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters :  97561939\n"
     ]
    }
   ],
   "source": [
    "## Counting the number of model parameters ##\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of parameters : ', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, epochs, criterion, optimizer, device):\n",
    "    \n",
    "    prev_val_loss = 0\n",
    "    \n",
    "    ## Tracking the accuracy and losses for plotting ##\n",
    "    \n",
    "    train_loss_tracker = []\n",
    "    val_loss_tracker = []\n",
    "    \n",
    "    train_value_tracker = {'humour':[], 'sarcasm': [], 'off':[], 'moti':[], 'senti':[]}\n",
    "    val_value_tracker = {'humour':[], 'sarcasm': [], 'off':[], 'moti':[], 'senti':[]}\n",
    "            \n",
    "    early_stopping = EarlyStopping()\n",
    "    \n",
    "    #model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "        train_items_total = 0\n",
    "        val_items_total = 0\n",
    "    \n",
    "        total_acc_train_humour = 0\n",
    "        total_acc_train_sarcasm = 0\n",
    "        total_acc_train_off = 0\n",
    "        total_acc_train_moti = 0\n",
    "        total_acc_train_senti = 0\n",
    "\n",
    "        total_acc_val_humour = 0\n",
    "        total_acc_val_sarcasm = 0\n",
    "        total_acc_val_off = 0\n",
    "        total_acc_val_moti = 0\n",
    "        total_acc_val_senti = 0\n",
    "\n",
    "        total_loss_val = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, img_input, _, train_label_sentiment, train_label_humour, train_label_sarcasm, train_label_offensive, train_label_motivational in tqdm(train_dataloader):\n",
    "\n",
    "            train_label_sentiment = train_label_sentiment.to(device)\n",
    "            train_label_humour = train_label_humour.to(device)\n",
    "            train_label_sarcasm = train_label_sarcasm.to(device)\n",
    "            train_label_offensive = train_label_offensive.to(device)\n",
    "            train_label_motivational = train_label_motivational.to(device)\n",
    "\n",
    "            img_input = img_input.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            model=model.to(device) ## don't comment this line -- very important\n",
    "\n",
    "            logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(input_id, mask, img_input)\n",
    "            #print(output.shape)\n",
    "            #print(train_label.long().shape)\n",
    "            ##print(train_label.reshape(2,1).long().shape)\n",
    "\n",
    "            batch_loss = criterion(logits_humour, train_label_humour.long()) + criterion(logits_sarcasm, train_label_sarcasm.long()) + criterion(logits_offensive, train_label_offensive.long()) + criterion(logits_motivational, train_label_motivational.long()) + criterion(logits_sentiment, train_label_sentiment.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc_humour = (logits_humour.argmax(dim=1) == train_label_humour).sum().item()\n",
    "            total_acc_train_humour += acc_humour\n",
    "#             print(total_acc_train_humour)\n",
    "#             print(acc_humour)\n",
    "\n",
    "            acc_sarcasm = (logits_sarcasm.argmax(dim=1) == train_label_sarcasm).sum().item()\n",
    "            total_acc_train_sarcasm += acc_sarcasm\n",
    "\n",
    "            acc_off = (logits_offensive.argmax(dim=1) == train_label_offensive).sum().item()\n",
    "            total_acc_train_off += acc_off\n",
    "\n",
    "            acc_moti = (logits_motivational.argmax(dim=1) == train_label_motivational).sum().item()\n",
    "            total_acc_train_moti += acc_moti\n",
    "\n",
    "            acc_senti = (logits_sentiment.argmax(dim=1) == train_label_sentiment).sum().item()\n",
    "            total_acc_train_senti += acc_senti\n",
    "            \n",
    "            ## number of training items in each batch ##\n",
    "            train_items_total += train_label_humour.shape[0]\n",
    "            #print(train_label_humour.shape[0])\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ## Eval ##\n",
    "\n",
    "        model.eval()\n",
    "        for train_input, img_input, _, val_label_sentiment, val_label_humour, val_label_sarcasm, val_label_offensive, val_label_motivational in val_dataloader:\n",
    "            val_label_sentiment = val_label_sentiment.to(device)\n",
    "            val_label_humour = val_label_humour.to(device)\n",
    "            val_label_sarcasm = val_label_sarcasm.to(device)\n",
    "            val_label_offensive = val_label_offensive.to(device)\n",
    "            val_label_motivational = val_label_motivational.to(device)\n",
    "\n",
    "            img_input = img_input.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(input_id, mask, img_input)\n",
    "\n",
    "            batch_loss = criterion(logits_humour, val_label_humour.long()) + criterion(logits_sarcasm, val_label_sarcasm.long()) + criterion(logits_offensive, val_label_offensive.long()) + criterion(logits_motivational, val_label_motivational.long()) + criterion(logits_sentiment, val_label_sentiment.long())\n",
    "            total_loss_val += batch_loss.item()\n",
    "\n",
    "            acc_humour = (logits_humour.argmax(dim=1) == val_label_humour).sum().item()\n",
    "            total_acc_val_humour += acc_humour\n",
    "            #print(total_acc_val_humour)\n",
    "\n",
    "            acc_sarcasm = (logits_sarcasm.argmax(dim=1) == val_label_sarcasm).sum().item()\n",
    "            total_acc_val_sarcasm += acc_sarcasm\n",
    "\n",
    "            acc_off = (logits_offensive.argmax(dim=1) == val_label_offensive).sum().item()\n",
    "            total_acc_val_off += acc_off\n",
    "\n",
    "            acc_moti = (logits_motivational.argmax(dim=1) == val_label_motivational).sum().item()\n",
    "            total_acc_val_moti += acc_moti\n",
    "\n",
    "            acc_senti = (logits_sentiment.argmax(dim=1) == val_label_sentiment).sum().item()\n",
    "            total_acc_val_senti += acc_senti\n",
    "            \n",
    "            ## number of validation items in each batch ##\n",
    "            val_items_total += val_label_humour.shape[0]\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} \\n | Train Loss: {total_loss_train / train_items_total: .3f} \\n\\n \\\n",
    "            | Train Accuracy (Humour): {total_acc_train_humour / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Sarcasm): {total_acc_train_sarcasm / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Offensive): {total_acc_train_off / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Motivational): {total_acc_train_moti / train_items_total: .3f} \\n \\\n",
    "            | Train Accuracy (Overall Sentiment): {total_acc_train_senti / train_items_total: .3f} \\n \\n \\n \\\n",
    "            | Val Loss: {total_loss_val / val_items_total: .3f} \\n\\n \\\n",
    "            | Val Accuracy (Humour): {total_acc_val_humour / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Sarcasm): {total_acc_val_sarcasm / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Offensive): {total_acc_val_off / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Motivational): {total_acc_val_moti / val_items_total: .3f} \\n\\\n",
    "            | Val Accuracy (Overall Sentiment): {total_acc_val_senti / val_items_total: .3f} \\n\\n\\n')\n",
    "\n",
    "        ## Adding to the list for plotting ##\n",
    "        train_loss_tracker.append(total_loss_train / train_items_total) \n",
    "        val_loss_tracker.append(total_loss_val / val_items_total)\n",
    "\n",
    "        train_value_tracker['humour'].append(total_acc_train_humour / train_items_total)\n",
    "        train_value_tracker['sarcasm'].append(total_acc_train_sarcasm / train_items_total)\n",
    "        train_value_tracker['off'].append(total_acc_train_off / train_items_total)\n",
    "        train_value_tracker['moti'].append(total_acc_train_moti / train_items_total)\n",
    "        train_value_tracker['senti'].append(total_acc_train_senti / train_items_total)\n",
    "\n",
    "        val_value_tracker['humour'].append(total_acc_val_humour / val_items_total)\n",
    "        val_value_tracker['sarcasm'].append(total_acc_val_sarcasm / val_items_total)\n",
    "        val_value_tracker['off'].append(total_acc_val_off / val_items_total)\n",
    "        val_value_tracker['moti'].append(total_acc_val_moti / val_items_total)\n",
    "        val_value_tracker['senti'].append(total_acc_val_senti / val_items_total)\n",
    "\n",
    "\n",
    "        ## Early stopping ##\n",
    "\n",
    "        if epoch_num != 0:\n",
    "            early_stopping(prev_val_loss, total_loss_val / val_items_total)\n",
    "            \n",
    "            current_val_loss = total_loss_val / val_items_total ## recently updated only in multimodal part\n",
    "            if current_val_loss < prev_val_loss: # if current value is less than the previous value\n",
    "                prev_val_loss = current_val_loss\n",
    "\n",
    "        elif epoch_num == 0:\n",
    "            prev_val_loss = total_loss_val / val_items_total\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Training stopped at epoch:\", epoch_num+1)\n",
    "            break\n",
    "                    \n",
    "    return train_loss_tracker, val_loss_tracker, train_value_tracker, val_value_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "lr = 1e-4 ## lr changed for multimodal model\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr= lr)\n",
    "\n",
    "train_loss_tracker, val_loss_tracker, train_value_tracker, val_value_tracker = train(model, train_dataloader, val_dataloader, epochs, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output: ##\n",
    "\n",
    "100%|██████████████████████████████████████████████████████████████████████████████| 1397/1397 [08:14<00:00,  2.83it/s]\n",
    "\n",
    "Epochs: 5 \n",
    " | Train Loss:  1.396 \n",
    " | Val Loss:  1.383\n",
    "\n",
    "\n",
    "\n",
    "Early stopping counter value:  3\n",
    "\n",
    "Training stopped at epoch: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test(model, test_dataloader, criterion, optimizer, device):\n",
    "    total_acc_test_humour = 0\n",
    "    total_acc_test_sarcasm = 0\n",
    "    total_acc_test_off = 0\n",
    "    total_acc_test_moti = 0\n",
    "    total_acc_test_senti = 0\n",
    "\n",
    "    total_loss_test = 0\n",
    "    \n",
    "    ## total test items in a dataloader ##\n",
    "    test_items_total = 0\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    img_lst =[]\n",
    "    predicted_labels_lst_hum=[]\n",
    "    orig_labels_lst_hum=[]\n",
    "    \n",
    "    predicted_labels_lst_sar=[]\n",
    "    orig_labels_lst_sar=[]\n",
    "    \n",
    "    predicted_labels_lst_moti=[]\n",
    "    orig_labels_lst_moti=[]\n",
    "    \n",
    "    predicted_labels_lst_off=[]\n",
    "    orig_labels_lst_off=[]\n",
    "    \n",
    "    predicted_labels_lst_senti=[]\n",
    "    orig_labels_lst_senti=[]\n",
    "    \n",
    "    for test_input, img_input, img_names, test_label_sentiment, test_label_humour, test_label_sarcasm, test_label_offensive, test_label_motivational in test_dataloader:\n",
    "        test_label_sentiment = test_label_sentiment.to(device)\n",
    "        test_label_humour = test_label_humour.to(device)\n",
    "        test_label_sarcasm = test_label_sarcasm.to(device)\n",
    "        test_label_offensive = test_label_offensive.to(device)\n",
    "        test_label_motivational = test_label_motivational.to(device)\n",
    "        \n",
    "        mask = test_input['attention_mask'].to(device)\n",
    "        input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        img_input = img_input.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_humour, logits_sarcasm, logits_offensive, logits_motivational, logits_sentiment = model(input_id, mask, img_input)\n",
    "            \n",
    "        img_lst.extend(list(img_names))\n",
    "#         print(test_label_sentiment.tolist())\n",
    "#         print(logits_humour.argmax(dim=1).tolist())\n",
    "#         print(img_names)\n",
    "        \n",
    "        predicted_labels_lst_hum.extend(logits_humour.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_hum.extend(test_label_humour.tolist())\n",
    "        \n",
    "        predicted_labels_lst_sar.extend(logits_sarcasm.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_sar.extend(test_label_sarcasm.tolist())\n",
    "\n",
    "        predicted_labels_lst_moti.extend(logits_motivational.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_moti.extend(test_label_motivational.tolist())\n",
    "\n",
    "        predicted_labels_lst_off.extend(logits_offensive.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_off.extend(test_label_offensive.tolist())\n",
    "\n",
    "        predicted_labels_lst_senti.extend(logits_sentiment.argmax(dim=1).tolist())\n",
    "        orig_labels_lst_senti.extend(test_label_sentiment.tolist())\n",
    "        \n",
    "        batch_loss = criterion(logits_humour, test_label_humour.long()) + criterion(logits_sarcasm, test_label_sarcasm.long()) + criterion(logits_offensive, test_label_offensive.long()) + criterion(logits_motivational, test_label_motivational.long()) + criterion(logits_sentiment, test_label_sentiment.long())\n",
    "        total_loss_test += batch_loss.item()\n",
    "\n",
    "        acc_humour = (logits_humour.argmax(dim=1) == test_label_humour).sum().item()\n",
    "        total_acc_test_humour += acc_humour\n",
    "\n",
    "        acc_sarcasm = (logits_sarcasm.argmax(dim=1) == test_label_sarcasm).sum().item()\n",
    "        total_acc_test_sarcasm += acc_sarcasm\n",
    "\n",
    "        acc_off = (logits_offensive.argmax(dim=1) == test_label_offensive).sum().item()\n",
    "        total_acc_test_off += acc_off\n",
    "\n",
    "        acc_moti = (logits_motivational.argmax(dim=1) == test_label_motivational).sum().item()\n",
    "        total_acc_test_moti += acc_moti\n",
    "\n",
    "        acc_senti = (logits_sentiment.argmax(dim=1) == test_label_sentiment).sum().item()\n",
    "        total_acc_test_senti += acc_senti\n",
    "        \n",
    "        ## number of test items in each batch ##\n",
    "        test_items_total += test_label_humour.shape[0]\n",
    "\n",
    "    print(\n",
    "        f'| Test Loss: {total_loss_test / test_items_total: .3f} \\n\\n \\\n",
    "        | Test Accuracy (Humour): {total_acc_test_humour / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Sarcasm): {total_acc_test_sarcasm / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Offensive): {total_acc_test_off / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Motivational): {total_acc_test_moti / test_items_total: .3f} \\n\\\n",
    "        | Test Accuracy (Overall Sentiment): {total_acc_test_senti / test_items_total: .3f} \\n\\n\\n')\n",
    "    \n",
    "    print('\\n\\n*********************Classification report***************\\n\\n')\n",
    "    print('Humour:')\n",
    "    print(classification_report(orig_labels_lst_hum, predicted_labels_lst_hum))\n",
    "    print('Sarcasm:')\n",
    "    print(classification_report(orig_labels_lst_sar, predicted_labels_lst_sar))\n",
    "    print('Offensive:')\n",
    "    print(classification_report(orig_labels_lst_off, predicted_labels_lst_off))\n",
    "    print('Motivational:')\n",
    "    print(classification_report(orig_labels_lst_moti, predicted_labels_lst_moti))\n",
    "    print('Overall Sentiment:')\n",
    "    print(classification_report(orig_labels_lst_senti, predicted_labels_lst_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_dataloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Output:</b> \n",
    "\n",
    "Test Accuracy (Humour): 34%\n",
    "\n",
    "Test Accuracy (Offensive): 39%\n",
    "\n",
    "Test Accuracy (Sarcasm): 48%\n",
    "\n",
    "Test Accuracy (Motivational): 65%\n",
    "\n",
    "Test Accuracy (Overall Sentiment): 48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model ###\n",
    "\n",
    "torch.save(model, './multimodal_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### charts ###\n",
    "\n",
    "## plot the charts - Accuracies -Train ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_hum = train_value_tracker['humour'].copy()\n",
    "y_pts_sar = train_value_tracker['sarcasm'].copy()\n",
    "y_pts_off = train_value_tracker['off'].copy()\n",
    "y_pts_moti = train_value_tracker['moti'].copy()\n",
    "y_pts_senti = train_value_tracker['senti'].copy()\n",
    "\n",
    "x_pts = [x for x in range(1,len(y_pts_hum)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_hum)\n",
    "plt.plot(x_pts,y_pts_sar)\n",
    "plt.plot(x_pts,y_pts_off)\n",
    "plt.plot(x_pts,y_pts_moti)\n",
    "plt.plot(x_pts,y_pts_senti)\n",
    "\n",
    "plt.title('Training chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.legend(['humour', 'sarcasm', 'offensive', 'motivational', 'sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the charts - Accuracies -Val ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_hum = val_value_tracker['humour'].copy()\n",
    "y_pts_sar = val_value_tracker['sarcasm'].copy()\n",
    "y_pts_off = val_value_tracker['off'].copy()\n",
    "y_pts_moti = val_value_tracker['moti'].copy()\n",
    "y_pts_senti = val_value_tracker['senti'].copy()\n",
    "\n",
    "x_pts = [x for x in range(1,len(y_pts_hum)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_hum)\n",
    "plt.plot(x_pts,y_pts_sar)\n",
    "plt.plot(x_pts,y_pts_off)\n",
    "plt.plot(x_pts,y_pts_moti)\n",
    "plt.plot(x_pts,y_pts_senti)\n",
    "\n",
    "plt.title('Training chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.legend(['humour', 'sarcasm', 'offensive', 'motivational', 'sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the charts - loss - train -Val ##\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_pts_loss_train = train_loss_tracker.copy()\n",
    "y_pts_loss_val = val_loss_tracker.copy()\n",
    "x_pts = [x for x in range(1,len(y_pts_loss_val)+1)]\n",
    "\n",
    "plt.plot(x_pts,y_pts_loss_train)\n",
    "plt.plot(x_pts,y_pts_loss_val)\n",
    "\n",
    "plt.title('Training and Validation Loss chart')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'val'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
